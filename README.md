# RPZHole-GenRPZ

Hosts list downloader and bind-dns update/synch script for RPZ blocking zones.

This script is a linux centric BASH list of commands to CLI tools for text processing. It uses a pre-set user-editable configuration for downloading and processing publicly available hostfile lists into an updatable list for synchronising to BIND dns server as a response-policy zone (RPZ). This means that all dns queries to the named service are filtered against the blocklist generated from the hostslists, without having to update and synch each network host.. or even for hosts that can't use custom hosts lists.

The script keeps the RPZ in synch with each addition and deletion of entries to the downloaded hostlists, as well as caching the downloaded files to save time and data during working. The synch to dns is done via an AXFR (domain transfer pull) from the DNS and use of TSIG authenticated nsupdate pushes of new/changed entries. Using the axfr allows the verification of deletions required to keep synch with the external sources. The name of the RPZ zone is arbitrary even relative to the rest of the zones on the DNS, and there are some very long target names in use. The max-name-length for bind is 255 characters so the longer your RPZ zone-name, the more long entries will cause failures in updates to the DNS. The minimum loss is 3 characters ( .X. first dot separator, the label character and the terminal root dot). The script attempts to handle internationalised domains as much as possible, but the scope of options in these types of domain is too large to be certain of full compatibility. The IDN conversion tool is also very, very slow for long lists of domains.

This means that there are a few installation steps. Firstly the creation of the RPZ zone in the BIND config, the TSIG authentication key for dns updates, and configuring for allowing zone-transfer to the update host. AXFR/nsupdate means that any network host can do the updates, but the traffic can be heavy (Currently have 12 hostfile sources in use, one of which is host-files.net.. and their list is about 600,000 entries - the rest add about 60,000 more.). I run the script on the dns host for direct access, but anywhere on a LAN/fast WAN should be fine. Internet updates are not reccommended.

The configuration files are in several parts. Firstly is the download URL list. This is passed to wget so each entry needs to be in standard URL format, with a direct link to the download needed. There is also a matching domain list which is added to the user whitelist to keep list sources from being blocked. Then there are the list of paths from the processing tmp folder to each content source. Some of the downloads come as zip-files or other multi-part files so you need to manually extract each source the first time to see where the actual hostfiles are extracted to, and the layout of the file. If the layout is a classic two-column (IP_address-name/name-IP_address) format the path entry goes to the 'classic-format' list. If the list is just bare domainnames it goes to the 'flat-sources' list. The 'complex-sources' list is for hostlists containing lots of extra information. So far the only entry uses the first column of the hostlist to store the target domainname entries, but other variations may exist also and require further updates as discovered. The 'match/unmatch' lists are target data to filter from the hosts list during processing. These may need editing if data is being missed or incorrectly applied to the synch list. Lastly there are the user white/black list files. these are for individual updates. The whitelist file is used as a regex source for grep so regex format matches can be used to filter multiple incorrect blocking entries. The blacklist is applied as a literal text list, so fully qualified plain text entries are needed here. The whitelist is applied last so duplicate entries in white and blacklists will be ALLOWED rather than blocked, also any entries in the automated lists will be over-ridden by whitelist entries.

There are options in the script header for applying list storage and cache storage paths and also the dns settings. The script has a couple of modes for scrubbing and zero-ing an existing RPZ if it has many un-needed entries, a rapid initialise mode for first startup and an error-checking mode for forcing through large changes in record size, when a list source is removed and no longer used. There is a cache scrubbing option also to remove stale files taken from unused hostfile sources. Add the script to cron or a systemd timer to update the entries daily.
